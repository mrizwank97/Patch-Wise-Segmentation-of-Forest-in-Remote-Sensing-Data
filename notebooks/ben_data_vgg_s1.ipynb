{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from skimage.io import imread\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import regularizers, optimizers\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_generator(files, batch_size=32):\n",
    "    from skimage.io import imread\n",
    "    from random import sample, choice\n",
    "    while True:\n",
    "        batch_files = sample(files, batch_size)\n",
    "        batch_Y = []\n",
    "        batch_X = []\n",
    "        for idx, input_path in enumerate(batch_files):\n",
    "            image = np.array(imread(input_path), dtype=float)[:,:,10:]\n",
    "            #image[:,:,0]= (image[:,:,0]-image[:,:,0].min())/(image[:,:,0].max()-image[:,:,0].min())\n",
    "            #image[:,:,1]= (image[:,:,1]-image[:,:,1].min())/(image[:,:,1].max()-image[:,:,1].min())\n",
    "            temp = input_path.split('/')[-1]\n",
    "            Y = list(df.loc[temp])\n",
    "            batch_Y += [Y]\n",
    "            batch_X += [image]\n",
    "        X = np.array(batch_X)\n",
    "        Y = np.array(batch_Y)\n",
    "        yield(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    return 2*((prec*rec)/(prec+rec+K.epsilon()))\n",
    "\n",
    "def build_callbacks():\n",
    "    checkpointer = ModelCheckpoint(filepath=\"../models/ben_data_vgg_s1.h5\", monitor='val_f1', verbose=1, save_best_only=True, save_weights_only=False, mode='max')\n",
    "    reduce = keras.callbacks.ReduceLROnPlateau(monitor='val_f1', factor=0.1, patience=4, mode='max')\n",
    "    early = keras.callbacks.EarlyStopping(monitor='val_f1', min_delta=1e-4, patience=15, mode='max')\n",
    "    csv = keras.callbacks.CSVLogger('../logs/ben_data_vgg_s1.csv', separator=',')\n",
    "    callbacks = [checkpointer, reduce, early, csv]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of images 254032\n",
      "Training Dataset Size 177822\n",
      "Validation Dataset Size 38105\n",
      "Test Dataset Size 38105\n"
     ]
    }
   ],
   "source": [
    "files = glob('/scratch/mrkhalid/ben_data'+\"/**/**/**/*.tif\")\n",
    "print('Total no. of images ' + str(len(files)))\n",
    "for i in range(100):\n",
    "    random.shuffle(files)\n",
    "ne = len(files)\n",
    "train_files = files[:int(.7*ne)]\n",
    "val_files = files[int(.7*ne):int(.85*ne)]\n",
    "test_files = files[int(.85*ne):ne]\n",
    "print('Training Dataset Size ' + str(len(train_files)))\n",
    "print('Validation Dataset Size ' + str(len(val_files)))\n",
    "print('Test Dataset Size ' + str(len(test_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'/scratch/mrkhalid/annotated.csv')\n",
    "df['ID'] = df['ID'] + '.tif'\n",
    "df.set_index(\"ID\", inplace=True)\n",
    "train_generator = image_generator(train_files, batch_size=32)\n",
    "val_generator = image_generator(val_files, batch_size=32)\n",
    "test_generator = image_generator(test_files, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0309 06:26:06.354669 139645529921344 deprecation_wrapper.py:119] From /home/mrkhalid/miniconda3/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0309 06:26:06.363479 139645529921344 deprecation.py:323] From /home/mrkhalid/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(120,120,2)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512,activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=512,activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=19, activation=\"sigmoid\"))\n",
    "model.compile(optimizers.adam(), loss=\"binary_crossentropy\", metrics=['accuracy', recall, precision, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrkhalid/miniconda3/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3120/5556 [===============>..............] - ETA: 21:15 - loss: 0.2675 - acc: 0.8812 - recall: 0.4528 - precision: 0.6780 - f1: 0.5406"
     ]
    }
   ],
   "source": [
    "train_steps = len(train_files) // 32\n",
    "val_steps = len(val_files) // 32\n",
    "test_steps =len(test_files) // 32\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=train_steps,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=100,\n",
    "                    callbacks = build_callbacks(),\n",
    "                    use_multiprocessing=True,\n",
    "                    max_queue_size = 512,\n",
    "                    workers=64\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc, rec, prec, f1 = model.evaluate_generator(test_generator,steps=test_steps)\n",
    "print(loss)\n",
    "print(acc)\n",
    "print(rec)\n",
    "print(prec)\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
